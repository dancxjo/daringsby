# Daringsby Workspace

This repository implements Pete Daringsby: a Rust-based artificial agent with real-time perception, emotional awareness, and expressive behavior. It is organized as a workspace with three crates:

* **`psyche`** ‚Äì core cognition (the mind of Pete)
* **`lingproc`** ‚Äì LLM and embedding abstraction layer
* **`pete`** ‚Äì the host binary, tying sensors and outputs to Pete‚Äôs cognitive loop

---

## üß† Architecture Overview

Pete's cognitive engine is structured as a sequence of `Wit` modules. Each Wit ingests lower-level impressions and emits higher-level `Impression<T>` thoughts. These are stored as `Experience<T>`s with vector embeddings.

Key concepts:

* `Sensation`: Raw input from sensors
* `Instant`: Narrative bundle of sensations
* `Impression<T>`: One-sentence summary with optional emotion
* `Experience<T>`: Stored impression with vector and ID

### Primary Wits

* **Quick**: Groups `Sensation`s into a coherent `Instant`
* **Combobulator**: Generates a concise `Impression` of what just happened
* **Memory**: Stores impressions in Neo4j and Qdrant
* **Heart**: Derives emotional state (emoji)
* **Will**: Issues behavioral instructions
* **Voice**: Generates responses when permitted

---

## üíª Example Usage

```rust
use lingproc::OllamaProvider;
use psyche::Psyche;

let narrator = OllamaProvider::new("http://localhost:11434", "mistral").unwrap();
let voice = OllamaProvider::new("http://localhost:11434", "mistral").unwrap();
let vectorizer = OllamaProvider::new("http://localhost:11434", "mistral").unwrap();

use psyche::{Ear, Mouth};
use async_trait::async_trait;

// Stub implementations for demonstration
struct DummyMouth;
#[async_trait] impl Mouth for DummyMouth { /* ... */ }

struct DummyEar;
#[async_trait] impl Ear for DummyEar { /* ... */ }

let psyche = Psyche::new(
    Box::new(narrator),
    Box::new(voice),
    Box::new(vectorizer),
    std::sync::Arc::new(psyche::NoopMemory),
    std::sync::Arc::new(DummyMouth),
    std::sync::Arc::new(DummyEar),
);
psyche.set_system_prompt("Respond in two sentences max.");
psyche.set_speak_when_spoken_to(true);
psyche.run().await;
```

---

## üöÄ Running the System

To run Pete with all services wired:

```sh
cargo run -p pete -- \
  --chatter-host http://localhost:11434 --chatter-model mistral \
  --wits-host http://localhost:11434 --wits-model mistral \
  --embeddings-host http://localhost:11434 --embeddings-model mistral \
  --qdrant-url http://localhost:6333 \
  --neo4j-uri bolt://localhost:7687 \
  --neo4j-user neo4j \
  --neo4j-pass password
```

To enable audio output with Coqui TTS:

```sh
cargo run -p pete --features tts -- \
  --tts-url http://localhost:5002/api/tts \
  --tts-speaker-id p123 \
  --tts-language-id en
```

---

## üåê Web Interface

Visit [`http://localhost:3000/`](http://localhost:3000/) after launch.

* WebSocket connection at `/ws`
* Debug info now included on `/ws` as `Think` events
* JSON endpoints:

  * `/conversation` ‚Äì full log
  * `/debug/psyche` ‚Äì tick stats

Events from Pete include speech, emotion changes, wit reports and conversation updates:

```json
{ "type": "say", "data": { "words": "hi", "audio": "..." } }
{ "type": "Emote", "data": "üòä" }
```

---

## üß™ Testing & Simulation

Run tests:

```sh
cargo test
```

Simulate input:

```sh
cargo run -p pete --bin simulate -- text "hello"
cargo run -p pete --bin simulate -- image some.png
```

---

## üîß Build Notes

* Format: `cargo fmt`
* Lint: `cargo clippy`
* Logging: `RUST_LOG=debug`
* Features: `tts`, `geo`, `eye`, `face`, `heartbeat`, `all-sensors`

---

## üìé Related Modules

* `lingproc::segment_text_into_sentences()` ‚Äì splits text:

```rust
let s = "Hello. World.";
let parts = lingproc::segment_text_into_sentences(s);
assert_eq!(parts, vec!["Hello.", "World."]);
```

---

## üß† Project Goals

* Maintain realism: Pete only speaks about what he perceives, remembers, or is told
* Modular cognition: composable Wits allow reuse and experimentation
* Agent narration: internal thoughts form a coherent and evolving identity

---

For deeper internals, see [`docs/Agent Overview`](./agents.md) or explore the `psyche::wits` module.
