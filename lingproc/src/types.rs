use anyhow::Result;
use async_trait::async_trait;
use once_cell::sync::Lazy;
use std::pin::Pin;
use tokio::sync::Mutex;
use tokio_stream::Stream;

/// Global context appended to future prompts.
static PROMPT_CONTEXT: Lazy<Mutex<Vec<String>>> = Lazy::new(|| Mutex::new(Vec::new()));

/// Add `note` for inclusion in the next prompt.
pub async fn push_prompt_context(note: &str) {
    PROMPT_CONTEXT.lock().await.push(note.to_string());
}

/// Consume and return all pending context notes.
pub async fn take_prompt_context() -> Vec<String> {
    PROMPT_CONTEXT.lock().await.drain(..).collect()
}

/// Represents an image passed into the instruction context.
///
/// `ImageData` is used to represent camera input, screen captures, or uploaded
/// assets. The `Doer` may include them in prompts for multimodal models (e.g.,
/// Gemini, GPT-4V).
#[derive(Debug, Clone)]
pub struct ImageData {
    pub mime: String,   // e.g., "image/png"
    pub base64: String, // base64-encoded content
}

/// Use `Instruction` to pass natural language plus multimedia context to the
/// `Doer`.
///
/// Example usage:
/// ```rust,ignore
/// let instruction = Instruction {
///     command: "Describe what you see.".to_string(),
///     images: vec![ImageData {
///         mime: "image/png".to_string(),
///         base64: capture_base64_image(), // <- User-defined
///     }],
/// };
/// let result = doer.follow(instruction).await?;
/// ```
///
/// In the future this struct may include audio or file attachments.
#[derive(Debug, Clone)]
pub struct Instruction {
    pub command: String,        // Natural language instruction
    pub images: Vec<ImageData>, // Optional supporting images
}

/// Trait for generating language model output from a structured prompt.
///
/// A `Doer` represents a general-purpose interface for calling an LLM using
/// structured input. It is used throughout Pete's cognitive system to turn a
/// prompt (e.g., a summary of recent perception) into a response string, which
/// may be a sayable utterance, internal decision tag, or reflection.
///
/// This was originally called an `InstructionFollower`, reflecting its role as a
/// functional LLM caller.
///
/// Implementations may stream or buffer LLM output, and may vary across
/// cognitive components.
///
/// # Example
/// ```rust
/// let result = doer.follow(Instruction {
///     command: "summarize what just happened".into(),
///     images: vec![],
/// }).await?;
/// println!("LLM says: {result}");
/// ```
#[async_trait]
pub trait Doer: Send + Sync {
    /// Follow an instruction, possibly with supporting images, and return the
    /// textual result.
    async fn follow(&self, instruction: Instruction) -> Result<String>;
}

/// Indicates the speaker of a message in a conversation.
///
/// `Role::User` = input from outside world.
/// `Role::Assistant` = output generated by Pete (via `Voice`).
#[derive(Clone, Debug, PartialEq, Eq)]
pub enum Role {
    Assistant,
    User,
}

/// Represents a single utterance in a chat history.
///
/// Used to maintain prompt history when calling the [`Chatter`] trait.
#[derive(Clone, Debug)]
pub struct Message {
    pub role: Role,
    pub content: String,
}

impl Message {
    /// Create a new user message.
    pub fn user(content: impl Into<String>) -> Self {
        Self {
            role: Role::User,
            content: content.into(),
        }
    }

    /// Create a new assistant message.
    pub fn assistant(content: impl Into<String>) -> Self {
        Self {
            role: Role::Assistant,
            content: content.into(),
        }
    }
}

/// An asynchronous stream of `Result<String>` response chunks from the LLM.
///
/// Used by [`Chatter::chat`]. Typically emits partial sentences or word
/// fragments. Terminates when the full response has been streamed.
pub type ChatStream = Pin<Box<dyn Stream<Item = Result<String>> + Send>>;

/// Trait for conversational generation. Produces a stream of natural language chunks.
///
/// This is used **exclusively** by the `Voice` Wit to generate speech content.
/// It receives a system prompt and conversation history, and returns a stream of
/// partial strings.
///
/// The returned [`ChatStream`] emits chunks progressively, enabling real-time
/// speech synthesis.
///
/// ## Example
/// ```rust,ignore
/// let stream = chatter.chat("You are Pete", &[Message::user("Hi")]).await?;
/// tokio::pin!(stream);
/// while let Some(chunk) = stream.next().await {
///     println!("LLM: {}", chunk?);
/// }
/// ```
///
/// The `Chatter` is responsible for turning a prompt + message history into a
/// streaming response.
///
/// The stream emits one chunk at a time (typically words or partial sentences).
/// These chunks may be:
///   - Buffered and split into full sentences
///   - Sent to TTS as soon as each sentence completes
///   - Annotated midstream with emotion or function tags (e.g., <function name="emote">ðŸ˜³</function>)
///
/// You can use `sentenceBySentence()` on the frontend or backend to process the stream.
///
/// This enables:
///   - Speaking mid-response
///   - Interruptibility
///   - Dynamic emotional expression
#[async_trait]
pub trait Chatter: Send + Sync {
    /// Start a chat session using `system_prompt` and `history`.
    ///
    /// Returns a stream of response chunks from the language model.
    async fn chat(&self, system_prompt: &str, history: &[Message]) -> Result<ChatStream>;

    /// Update additional context for future prompts.
    ///
    /// The default implementation appends `context` to the prompt string used on
    /// the next [`chat`] call.
    async fn update_prompt_context(&self, context: &str) {
        push_prompt_context(context).await;
    }
}

/// Trait for generating semantic vector embeddings from text.
///
/// Used for memory indexing into Qdrant. The input text should be one complete
/// sentence representing an [`Impression`] headline.
#[async_trait]
pub trait Vectorizer: Send + Sync {
    /// Convert `text` into a vector representation suitable for similarity search.
    async fn vectorize(&self, text: &str) -> Result<Vec<f32>>;
}
