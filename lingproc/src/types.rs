use anyhow::Result;
use async_trait::async_trait;
use once_cell::sync::Lazy;
use serde::{Deserialize, Serialize};
use std::pin::Pin;
use tokio::sync::Mutex;
use tokio_stream::Stream;
#[cfg(feature = "ts")]
use ts_rs::TS;

/// Global context appended to future prompts.
static PROMPT_CONTEXT: Lazy<Mutex<Vec<String>>> = Lazy::new(|| Mutex::new(Vec::new()));

/// Add `note` for inclusion in the next prompt.
///
/// Notes are buffered globally and appended to the system prompt the next
/// time [`Chatter::chat`](crate::types::Chatter::chat) is called. They are
/// removed once [`take_prompt_context`] is invoked.
///
/// # Example
/// ```rust,ignore
/// use lingproc::{push_prompt_context, take_prompt_context};
///
/// # tokio_test::block_on(async {
/// push_prompt_context("remember the user's name").await;
/// assert_eq!(
///     take_prompt_context().await,
///     vec!["remember the user's name".to_string()]
/// );
/// # });
/// ```
pub async fn push_prompt_context(note: &str) {
    PROMPT_CONTEXT.lock().await.push(note.to_string());
}

/// Consume and return all pending context notes.
///
/// The returned vector contains every note previously added via
/// [`push_prompt_context`] in insertion order. Calling this function clears the
/// buffer so that subsequent calls yield an empty vector until more notes are
/// pushed.
///
/// # Example
/// ```rust,ignore
/// use lingproc::{push_prompt_context, take_prompt_context};
///
/// # tokio_test::block_on(async {
/// push_prompt_context("A").await;
/// push_prompt_context("B").await;
/// let notes = take_prompt_context().await;
/// assert_eq!(notes, vec!["A".to_string(), "B".to_string()]);
/// assert!(take_prompt_context().await.is_empty());
/// # });
/// ```
pub async fn take_prompt_context() -> Vec<String> {
    PROMPT_CONTEXT.lock().await.drain(..).collect()
}

/// Represents an image passed into the instruction context.
///
/// `ImageData` is used to represent camera input, screen captures, or uploaded
/// assets. The `Doer` may include them in prompts for multimodal models (e.g.,
/// Gemini, GPT-4V).
#[cfg_attr(feature = "ts", derive(TS))]
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ImageData {
    pub mime: String,   // e.g., "image/png"
    pub base64: String, // base64-encoded content
}

/// Use `LlmInstruction` to pass natural language plus multimedia context to the
/// `Doer`.
///
/// Example usage:
/// ```rust,ignore
/// let instruction = LlmInstruction {
///     command: "Describe what you see.".to_string(),
///     images: vec![ImageData {
///         mime: "image/png".to_string(),
///         base64: capture_base64_image(), // <- User-defined
///     }],
/// };
/// let result = doer.follow(instruction).await?;
/// ```
///
/// In the future this struct may include audio or file attachments.
#[derive(Debug, Clone)]
pub struct LlmInstruction {
    pub command: String,        // Natural language instruction
    pub images: Vec<ImageData>, // Optional supporting images
}

/// Trait for generating language model output from a structured prompt.
///
/// A `Doer` represents a general-purpose interface for calling an LLM using
/// structured input. It is used throughout Pete's cognitive system to turn a
/// prompt (e.g., a summary of recent perception) into a response string, which
/// may be a sayable utterance, internal decision tag, or reflection.
///
/// This was originally called an `InstructionFollower`, reflecting its role as a
/// functional LLM caller.
///
/// Implementations may stream or buffer LLM output, and may vary across
/// cognitive components.
///
/// # Example
/// ```rust,ignore
/// let result = doer.follow(LlmInstruction {
///     command: "summarize what just happened".into(),
///     images: vec![],
/// }).await?;
/// println!("LLM says: {result}");
/// ```
#[async_trait]
pub trait Doer: Send + Sync {
    /// Follow an instruction, possibly with supporting images, and return the
    /// textual result.
    async fn follow(&self, instruction: LlmInstruction) -> Result<String>;
}

/// LLM-synthesized decision extracted from model output.
///
/// The generic parameter `I` represents a parsed instruction type. In the
/// broader PETE system this maps to `psyche::Instruction`.
#[cfg_attr(feature = "ts", derive(TS))]
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq)]
pub struct Decision<I> {
    /// Raw text returned by the language model.
    pub text: String,
    /// Structured instructions extracted from `text`.
    pub instructions: Vec<I>,
}

/// Indicates the speaker of a message in a conversation.
///
/// `Role::User` = input from outside world.
/// `Role::Assistant` = output generated by Pete (via `Voice`).
#[derive(Clone, Debug, PartialEq, Eq)]
pub enum Role {
    Assistant,
    User,
}

/// Represents a single utterance in a chat history.
///
/// Used to maintain prompt history when calling the [`Chatter`] trait.
#[derive(Clone, Debug)]
pub struct Message {
    pub role: Role,
    pub content: String,
}

impl Message {
    /// Create a new user message.
    pub fn user(content: impl Into<String>) -> Self {
        Self {
            role: Role::User,
            content: content.into(),
        }
    }

    /// Create a new assistant message.
    pub fn assistant(content: impl Into<String>) -> Self {
        Self {
            role: Role::Assistant,
            content: content.into(),
        }
    }
}

/// An asynchronous stream of textual chunks.
///
/// The error type defaults to [`anyhow::Error`], matching most provider
/// implementations.  Use it whenever streaming strings.
pub type TextStream<E = anyhow::Error> = Pin<Box<dyn Stream<Item = Result<String, E>> + Send>>;

/// Backwards-compatibility alias.
#[deprecated(note = "use `TextStream` instead")]
pub type ChatStream = TextStream;

/// Trait for conversational generation. Produces a stream of natural language chunks.
///
/// This is used **exclusively** by the `Voice` Wit to generate speech content.
/// It receives a system prompt and conversation history, and returns a stream of
/// partial strings.
///
/// The returned [`TextStream`] emits chunks progressively, enabling real-time
/// speech synthesis.
///
/// ## Example
/// ```rust,ignore
/// let stream = chatter.chat("You are Pete", &[Message::user("Hi")]).await?;
/// tokio::pin!(stream);
/// while let Some(chunk) = stream.next().await {
///     println!("LLM: {}", chunk?);
/// }
/// ```
///
/// The `Chatter` is responsible for turning a prompt + message history into a
/// streaming response.
///
/// The stream emits one chunk at a time (typically words or partial sentences).
/// These chunks may be:
///   - Buffered and split into full sentences
///   - Sent to TTS as soon as each sentence completes
///   - Annotated midstream with emotion or function tags (e.g., <function name="emote">ðŸ˜³</function>)
///
/// You can use `sentenceBySentence()` on the frontend or backend to process the stream.
///
/// This enables:
///   - Speaking mid-response
///   - Interruptibility
///   - Dynamic emotional expression
#[async_trait]
pub trait Chatter: Send + Sync {
    /// Start a chat session using `system_prompt` and `history`.
    ///
    /// Returns a stream of response chunks from the language model.
    async fn chat(&self, system_prompt: &str, history: &[Message]) -> Result<TextStream>;

    /// Update additional context for future prompts.
    ///
    /// The default implementation appends `context` to the prompt string used on
    /// the next [`chat`] call.
    async fn update_prompt_context(&self, context: &str) {
        push_prompt_context(context).await;
    }
}

/// Trait for generating semantic vector embeddings from text.
///
/// Used for memory indexing into Qdrant. The input text should be one complete
/// sentence representing an [`Impression`] headline.
#[async_trait]
pub trait Vectorizer: Send + Sync {
    /// Convert `text` into a vector representation suitable for similarity search.
    async fn vectorize(&self, text: &str) -> Result<Vec<f32>>;
}
