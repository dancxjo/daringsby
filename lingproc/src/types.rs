use anyhow::Result;
use async_trait::async_trait;
use std::pin::Pin;
use tokio_stream::Stream;

/// Represents an image passed into the instruction context.
///
/// `ImageData` is used to represent camera input, screen captures, or uploaded
/// assets. The `Doer` may include them in prompts for multimodal models (e.g.,
/// Gemini, GPT-4V).
#[derive(Debug, Clone)]
pub struct ImageData {
    pub mime: String,   // e.g., "image/png"
    pub base64: String, // base64-encoded content
}

/// Use `Instruction` to pass natural language plus multimedia context to the
/// `Doer`.
///
/// Example usage:
/// ```rust,ignore
/// let instruction = Instruction {
///     command: "Describe what you see.".to_string(),
///     images: vec![ImageData {
///         mime: "image/png".to_string(),
///         base64: capture_base64_image(), // <- User-defined
///     }],
/// };
/// let result = doer.follow(instruction).await?;
/// ```
///
/// In the future this struct may include audio or file attachments.
#[derive(Debug, Clone)]
pub struct Instruction {
    pub command: String,        // Natural language instruction
    pub images: Vec<ImageData>, // Optional supporting images
}

/// Trait for executing imperative instructions (e.g., take a photo, run a Cypher query).
///
/// Used by the `Will` to invoke external side effects or decisions via LLM-generated
/// commands.
///
/// ## Example
/// ```rust,ignore
/// let output = doer
///     .follow(Instruction { command: "take a photo".into(), images: vec![] })
///     .await?;
/// println!("Output: {output}");
/// ```
#[async_trait]
pub trait Doer: Send + Sync {
    /// Follow an instruction, possibly with supporting images, and return the
    /// textual result.
    ///
    /// Implementors may call an external LLM or other service.
    async fn follow(&self, instruction: Instruction) -> Result<String>;
}

/// Indicates the speaker of a message in a conversation.
///
/// `Role::User` = input from outside world.
/// `Role::Assistant` = output generated by Pete (via `Voice`).
#[derive(Clone, Debug, PartialEq, Eq)]
pub enum Role {
    Assistant,
    User,
}

/// Represents a single utterance in a chat history.
///
/// Used to maintain prompt history when calling the [`Chatter`] trait.
#[derive(Clone, Debug)]
pub struct Message {
    pub role: Role,
    pub content: String,
}

impl Message {
    /// Create a new user message.
    pub fn user(content: impl Into<String>) -> Self {
        Self {
            role: Role::User,
            content: content.into(),
        }
    }

    /// Create a new assistant message.
    pub fn assistant(content: impl Into<String>) -> Self {
        Self {
            role: Role::Assistant,
            content: content.into(),
        }
    }
}

/// An asynchronous stream of `Result<String>` response chunks from the LLM.
///
/// Used by [`Chatter::chat`]. Typically emits partial sentences or word
/// fragments. Terminates when the full response has been streamed.
pub type ChatStream = Pin<Box<dyn Stream<Item = Result<String>> + Send>>;

/// Trait for conversational generation. Produces a stream of natural language chunks.
///
/// This is used **exclusively** by the `Voice` Wit to generate speech content.
/// It receives a system prompt and conversation history, and returns a stream of
/// partial strings.
///
/// The returned [`ChatStream`] emits chunks progressively, enabling real-time
/// speech synthesis.
///
/// ## Example
/// ```rust,ignore
/// let stream = chatter.chat("You are Pete", &[Message::user("Hi")]).await?;
/// tokio::pin!(stream);
/// while let Some(chunk) = stream.next().await {
///     println!("LLM: {}", chunk?);
/// }
/// ```
///
/// The `Chatter` is responsible for turning a prompt + message history into a
/// streaming response.
///
/// The stream emits one chunk at a time (typically words or partial sentences).
/// These chunks may be:
///   - Buffered and split into full sentences
///   - Sent to TTS as soon as each sentence completes
///   - Annotated midstream with emotion or function tags (e.g., <function name="emote">ðŸ˜³</function>)
///
/// You can use `sentenceBySentence()` on the frontend or backend to process the stream.
///
/// This enables:
///   - Speaking mid-response
///   - Interruptibility
///   - Dynamic emotional expression
#[async_trait]
pub trait Chatter: Send + Sync {
    /// Start a chat session using `system_prompt` and `history`.
    ///
    /// Returns a stream of response chunks from the language model.
    async fn chat(&self, system_prompt: &str, history: &[Message]) -> Result<ChatStream>;
}

/// Trait for generating semantic vector embeddings from text.
///
/// Used for memory indexing into Qdrant. The input text should be one complete
/// sentence representing an [`Impression`] headline.
#[async_trait]
pub trait Vectorizer: Send + Sync {
    /// Convert `text` into a vector representation suitable for similarity search.
    async fn vectorize(&self, text: &str) -> Result<Vec<f32>>;
}
